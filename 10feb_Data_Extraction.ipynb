{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlBtfOCd/DBMdZ8Q3WlyPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhinaytechie/Excerl/blob/main/10feb_Data_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwe2YKb4Rodi",
        "outputId": "121fd8f5-2b95-45cf-c5ff-ba084fdfb7c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPdf2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/232.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPdf2\n",
            "Successfully installed PyPdf2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPdf2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "HW-BMUeHSDlB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf=open(\"/content/sample_data/Abhinay Resume.pdf\",\"rb\")\n",
        "pdf_reader=PyPDF2.PdfReader(pdf)\n",
        "print(\"number of pages:\",len(pdf_reader.pages))\n",
        "page=pdf_reader.pages[0]\n",
        "print(page.extract_text())\n",
        "pdf.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UgB6ML6STN_",
        "outputId": "fbd2bea1-28b1-437f-a6a5-9ed8cc66b03c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of pages: 1\n",
            "Bhargava Sai Abhinay Bondalapati  \n",
            "Hyderabad, Telangana • +91 79 8910 4567 • bhargavasaiabhinay.b@gmail.com   \n",
            " Linkedin- hƩps://www.linkedin.com/in/abhinaybondalapa Ɵ/ \n",
            " \n",
            "PROFILE SUMMARY \n",
            "CreaƟve and versa Ɵle B.Tech student with a par Ɵcular liking for the ﬁeld of ar Ɵﬁcial intelligence . Takes part in compe ƟƟve programming \n",
            "and coding contests. Has a understanding of Data Structures and Algorithms and impleme ntaƟon of it in solving various problems and \n",
            "works well with others I have set high standards and plan to use my technical knowledge and innova Ɵve approach in the ﬁeld of Ar Ɵﬁcial \n",
            "Intelligence. I am seeking a star Ɵng posiƟon as an AI Engineer to leverage my DSA exper Ɵse and learn from seasoned profess ionals in the \n",
            "ﬁeld. \n",
            "SKILLS \n",
            " \n",
            "Front End : Cascading Style Sheet, Hypertext Markup Language, JavaScript, React.js \n",
            "Back End: Java, MySQL, Python, DSA \n",
            "SoŌ Skills: Communica Ɵon, Problem Solving, Time Management  \n",
            "EDUCATION \n",
            " \n",
            "Malla Reddy University \n",
            "B.Tech Computer Science & Engineering • Jan 2022 - Dec 2026  \n",
            "Sri chaitanya College  \n",
            "MPC • ECIL ,Hyderabad • GPA: 9.6 • Jul 2020 - Jul 2022 \n",
            "Gowtham Model School \n",
            "• ECIL ,Hyderabad • GPA: 10 • May 2020 \n",
            "PROJECTS \n",
            " \n",
            "BlogWebsite • Jun 2024 - Jul 2024 \n",
            "A BlogWebsite to describe my lifestyle and personality using Full stack web development tools. \n",
            "SalesPro-Insight • Mar 2024 - Jun 2024 \n",
            "MallaReddyUniversity \n",
            "This project demonstrates my ability to leverage Machine learning techniques to solve real-world business challenges and drive \n",
            "acƟonable insights for informed decision -making \n",
            "Porfolio Website • May 2024 - Jun 2024 \n",
            "I have created my personal por ƞolio using Html, Css and javascript which dive into my personal achievements and skills . \n",
            "CERTIFICATIONS \n",
            " \n",
            "The Web Developer Bootcamp 2024  \n",
            "Udemy \n",
            "AI for Everyone  \n",
            "Coursera \n",
            "Core java  \n",
            "Internshala \n",
            "Programming in java  \n",
            "NPTEL \n",
            "Scalable Machine Learning \n",
            "Coursera \n",
            " \n",
            "ACHIVEMENTS \n",
            " \n",
            " NPTEL(Programming in java)  \n",
            "     -Acquired Elite Silver Cer ƟﬁcaƟon with 78% \n",
            " Social Summer of Code Contributor \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2,urllib,nltk\n",
        "from io import BytesIO\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "SYCX8zLiS96l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk1F0_A4VXth",
        "outputId": "483c60c1-0755-4d53-abcd-cdaf524ad206"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wFile=urllib.request.urlopen(\"https://www.udri.org/pdf/02%20working%20paper%201.pdf\")\n",
        "pdfreader=PyPDF2.PdfReader(BytesIO(wFile.read()))"
      ],
      "metadata": {
        "id": "ZIjxZ0KOVYGf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pageObj=pdfreader.pages[1]\n",
        "page2=pageObj.extract_text()\n",
        "#Cleaning the text\n",
        "punctuations=['(',')',';',':','[',']',',','...','-' ]\n",
        "tokens=word_tokenize(page2)\n",
        "stop_words=stopwords.words('english')\n",
        "keywords=[word for word in tokens if not word in stop_words and not word in punctuations]"
      ],
      "metadata": {
        "id": "zGn1dleSVjki"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDWppBUVXXSV",
        "outputId": "93461f9f-3d6e-422b-daf4-6996eec02c26"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '2014‐2034',\n",
              " 'Acknowledgements',\n",
              " 'The',\n",
              " 'Consultant',\n",
              " 'wishes',\n",
              " 'thank',\n",
              " 'following',\n",
              " 'individuals',\n",
              " 'Municipal',\n",
              " 'Corporation',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " 'invaluable',\n",
              " 'support',\n",
              " 'insights',\n",
              " 'contributions',\n",
              " 'towards',\n",
              " '‘',\n",
              " 'Working',\n",
              " 'Paper',\n",
              " '1',\n",
              " '–',\n",
              " 'Preparation',\n",
              " 'Base',\n",
              " 'Map',\n",
              " '’',\n",
              " 'preparation',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '2014‐34',\n",
              " '.',\n",
              " '\\uf0a7',\n",
              " 'Mr.',\n",
              " 'Subodh',\n",
              " 'Kumar',\n",
              " 'IAS',\n",
              " 'Municipal',\n",
              " 'Commissioner',\n",
              " '\\uf0a7',\n",
              " 'Mr.',\n",
              " 'Rajeev',\n",
              " 'Kuknoor',\n",
              " 'Chief',\n",
              " 'Engineer',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " '\\uf0a7',\n",
              " 'Mr.',\n",
              " 'Sudhir',\n",
              " 'Ghate',\n",
              " 'Deputy',\n",
              " 'Chief',\n",
              " 'Engineer',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " '\\uf0a7',\n",
              " 'Mr.',\n",
              " 'A.G.',\n",
              " 'Marathe',\n",
              " 'Deputy',\n",
              " 'Chief',\n",
              " 'Engineer',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " '\\uf0a7',\n",
              " 'Mr.',\n",
              " 'R.',\n",
              " 'Balachandran',\n",
              " 'Executive',\n",
              " 'Engineer',\n",
              " 'Town',\n",
              " 'Planning',\n",
              " 'Officer',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " '.',\n",
              " 'Our',\n",
              " 'gratitude',\n",
              " 'following',\n",
              " 'experts',\n",
              " 'invaluable',\n",
              " 'insights',\n",
              " 'support',\n",
              " '\\uf0a7',\n",
              " 'Mr.',\n",
              " 'V.K',\n",
              " 'Phatak',\n",
              " 'Former',\n",
              " 'Chief',\n",
              " 'Town',\n",
              " 'Planner',\n",
              " 'MMRDA',\n",
              " '\\uf0a7',\n",
              " 'Mr.',\n",
              " 'A.N',\n",
              " 'Kale',\n",
              " 'Former',\n",
              " 'Chief',\n",
              " 'Engineer',\n",
              " 'DP',\n",
              " '\\uf0a7',\n",
              " 'Mr.',\n",
              " 'A.',\n",
              " 'S',\n",
              " 'Jain',\n",
              " 'Former',\n",
              " 'Dy.',\n",
              " 'Chief',\n",
              " 'Engineer',\n",
              " 'DP',\n",
              " '.',\n",
              " 'We',\n",
              " 'wish',\n",
              " 'especially',\n",
              " 'thank',\n",
              " 'MCGM',\n",
              " 'officers',\n",
              " 'Mr.',\n",
              " 'Jagdish',\n",
              " 'Talreja',\n",
              " 'Mr.',\n",
              " 'Dinesh',\n",
              " 'Naik',\n",
              " 'Mr.',\n",
              " 'Hiren',\n",
              " 'Daftardar',\n",
              " 'Ms.',\n",
              " 'Anita',\n",
              " 'Naik',\n",
              " 'continual',\n",
              " 'support',\n",
              " 'since',\n",
              " 'beginning',\n",
              " 'project',\n",
              " 'help',\n",
              " 'towards',\n",
              " 'familiarization',\n",
              " 'data',\n",
              " 'collection',\n",
              " '.',\n",
              " 'They',\n",
              " 'instrumental',\n",
              " 'helping',\n",
              " 'contact',\n",
              " 'various',\n",
              " 'MCGM',\n",
              " 'departments',\n",
              " 'well',\n",
              " 'helping',\n",
              " 'establish',\n",
              " 'contact',\n",
              " 'personnel',\n",
              " 'government',\n",
              " 'departments',\n",
              " 'organizations',\n",
              " '.',\n",
              " 'Many',\n",
              " 'thanks',\n",
              " 'MCGM',\n",
              " 'team',\n",
              " 'deploying',\n",
              " 'personnel',\n",
              " 'particularly',\n",
              " 'Mr.',\n",
              " 'Prasad',\n",
              " 'Gharat',\n",
              " 'extensive',\n",
              " 'field',\n",
              " 'visits',\n",
              " 'helped',\n",
              " 'understanding',\n",
              " 'actual',\n",
              " 'ground',\n",
              " 'conditions',\n",
              " '.',\n",
              " 'We',\n",
              " 'apologize',\n",
              " 'inadvertently',\n",
              " 'omitted',\n",
              " 'anyone',\n",
              " 'acknowledgement',\n",
              " 'due',\n",
              " '.',\n",
              " 'We',\n",
              " 'hope',\n",
              " 'anticipate',\n",
              " 'work',\n",
              " \"'s\",\n",
              " 'usefulness',\n",
              " 'intended',\n",
              " 'purpose',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name_list=list()\n",
        "check=['Mr.','Mrs.','Ms.']\n",
        "for idx,token in enumerate(keywords):\n",
        "  if token.startswith(tuple(check)) and idx<(len(keywords)-1):\n",
        "    name=token+keywords[idx+1]+' '+keywords[idx+2]\n",
        "    name_list.append(name)\n",
        "print(name_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLz1bp7kXgdR",
        "outputId": "555ff6df-4eeb-491e-f0d9-7ca3b75b1f3b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mr.Subodh Kumar', 'Mr.Rajeev Kuknoor', 'Mr.Sudhir Ghate', 'Mr.A.G. Marathe', 'Mr.R. Balachandran', 'Mr.V.K Phatak', 'Mr.A.N Kale', 'Mr.A. S', 'Mr.Jagdish Talreja', 'Mr.Dinesh Naik', 'Mr.Hiren Daftardar', 'Ms.Anita Naik', 'Mr.Prasad Gharat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wFile.close()"
      ],
      "metadata": {
        "id": "276Pi7x7Y6A2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collecting Data From Word Files**"
      ],
      "metadata": {
        "id": "3JDA7j4aaJEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz_wW6WqY8z5",
        "outputId": "c609bada-0d25-46ce-c9ae-b4ba9a903c90"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/244.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx"
      ],
      "metadata": {
        "id": "KSqx4OUWaR6L"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc=open(\"/content/sample_data/A14 litertaure.docx\",\"rb\")\n",
        "document=docx.Document(doc)\n",
        "docu=\"\"\n",
        "for para in document.paragraphs:\n",
        "  docu+=para.text\n",
        "print(docu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3bR4ZNZaUgt",
        "outputId": "15fdf59c-b5fb-47df-9c0a-0b19e22edf8d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "III Year B.Tech. - I Semester       Name of Examination: Application Development – Natural Language Processing Explore Course Code: MR22-1CS0264 Section: ALPHA (AT-14) ---------------------------------------------------------------------------- Literature Survey  Resume Parcel Using Tokenization, Named Entity Recognition, and Text Classification Reference1: Towards Efficient Resume Understanding: A Multi-Granularity Multi-Modal      Pre-Training Approach\" by Feihu Jiang et al. (2024)\n",
            "This paper explores the use of Named Entity Recognition (NER) and Part-of-Speech (POS) tagging to extract key details from resumes. The authors employ a hybrid approach, combining rule-based methods with machine learning techniques like Conditional Random Fields (CRFs) to identify structured information such as names, skills, and education. By leveraging linguistic patterns and contextual embeddings, the model effectively parses structured and semi-structured resumes while minimizing errors in entity recognition.The proposed system demonstrates strong performance, achieving an F1-score of 0.89 for entity extraction. The combination of rule-based techniques and CRFs significantly improves accuracy, outperforming standalone machine learning models. The study highlights how hybrid approaches offer enhanced generalization, making them effective for parsing diverse resume formats.Reference2: \"Resume Parsing using spaCy\" by Vikrant Patil (2022)\n",
            "This research dives into deep learning-based resume parsing, focusing on Bidirectional LSTMs and Transformer models such as BERT. The authors train pre-trained language models to recognize and classify resume sections, including work experience, education, and skills. By fine-tuning BERT with domain-specific corpora, the model captures variations in resume structures and effectively understands contextual relationships between different sections.The results demonstrate that the BERT-based model achieves a classification accuracy of 92.5%, significantly outperforming traditional machine learning methods. Transfer learning proves essential for handling unstructured resumes, allowing the model to adapt to different writing styles. The study emphasizes the superiority of deep learning in parsing complex textual data, making it a viable approach for large-scale resume processing.Reference 3: End-to-End Resume Parsing and Finding Candidates for a Job Description using BERT\" by Vedant Bhatia et al. (2019)\n",
            "This paper presents a rule-based method for resume parsing, employing predefined templates and regular expressions to extract structured data. The system is designed to recognize standard resume formats and identify key sections such as contact information, education, and skills. By leveraging manually crafted heuristics, the approach ensures efficient text segmentation and classification.Despite achieving an accuracy of 85%, the rule-based system faces challenges when parsing highly unstructured or visually rich resumes. While it performs well on conventional formats, its scalability is limited due to its reliance on fixed patterns. The study underscores the need for adaptive models that can generalize across various resume styles.Reference4: \"Resume Parsing Using Machine Learning: A Comparative Study\"\n",
            "This study compares different machine learning models, including Support Vector Machines (SVMs), Random Forests, and Conditional Random Fields (CRFs), for resume parsing. Feature engineering techniques, such as n-grams and word embeddings, are used to enhance model performance. The research explores the impact of contextual representations in improving information extraction accuracy.Among the tested models, CRFs yield the highest F1-score of 0.91, particularly excelling in extracting job titles and skills. The study highlights the importance of feature selection in refining machine learning models for resume parsing. While CRFs offer robust performance, they require extensive domain-specific preprocessing to achieve optimal results.Reference5 : Case Study: NLP Based Resume Parser Using BERT in Python\"byPragnakalpTechlabs(2022)\n",
            "This paper introduces an automated pipeline for resume parsing that integrates NLP methods such as tokenization, lemmatization, and dependency parsing. The approach also incorporates clustering algorithms to group similar resumes, along with topic modeling techniques to extract key themes and skills from the text.The system achieves a precision of 88% in matching resumes with job descriptions, demonstrating its effectiveness in recruitment automation. Clustering proves useful in handling large datasets, while topic modeling provides deeper insights into candidate qualifications. The study underscores the growing role of NLP in optimizing hiring processes and improving candidate-job alignment.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(document.paragraphs)):\n",
        "  print(\"The content of the pargaph \"+str(i)+\"is: \"+document.paragraphs[i].text+\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQHmcCgWamXB",
        "outputId": "7393bc6f-6011-4f5c-c147-6a874c71129a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The content of the pargaph 0is: III Year B.Tech. - I Semester \n",
            "\n",
            "The content of the pargaph 1is:       Name of Examination: Application Development – Natural Language Processing Explore \n",
            "\n",
            "The content of the pargaph 2is: Course Code: MR22-1CS0264 \n",
            "\n",
            "The content of the pargaph 3is: Section: ALPHA (AT-14) \n",
            "\n",
            "The content of the pargaph 4is: ---------------------------------------------------------------------------- \n",
            "\n",
            "The content of the pargaph 5is: Literature Survey \n",
            "\n",
            "The content of the pargaph 6is:  Resume Parcel Using Tokenization, Named Entity Recognition, and Text Classification\n",
            "\n",
            "The content of the pargaph 7is:  \n",
            "\n",
            "The content of the pargaph 8is: Reference1: Towards Efficient Resume Understanding: A Multi-Granularity Multi-Modal      Pre-Training Approach\" by Feihu Jiang et al. (2024)\n",
            "\n",
            "The content of the pargaph 9is: \n",
            "This paper explores the use of Named Entity Recognition (NER) and Part-of-Speech (POS) tagging to extract key details from resumes. The authors employ a hybrid approach, combining rule-based methods with machine learning techniques like Conditional Random Fields (CRFs) to identify structured information such as names, skills, and education. By leveraging linguistic patterns and contextual embeddings, the model effectively parses structured and semi-structured resumes while minimizing errors in entity recognition.\n",
            "\n",
            "The content of the pargaph 10is: \n",
            "\n",
            "The content of the pargaph 11is: The proposed system demonstrates strong performance, achieving an F1-score of 0.89 for entity extraction. The combination of rule-based techniques and CRFs significantly improves accuracy, outperforming standalone machine learning models. The study highlights how hybrid approaches offer enhanced generalization, making them effective for parsing diverse resume formats.\n",
            "\n",
            "The content of the pargaph 12is: \n",
            "\n",
            "The content of the pargaph 13is: Reference2: \"Resume Parsing using spaCy\" by Vikrant Patil (2022)\n",
            "\n",
            "The content of the pargaph 14is: \n",
            "This research dives into deep learning-based resume parsing, focusing on Bidirectional LSTMs and Transformer models such as BERT. The authors train pre-trained language models to recognize and classify resume sections, including work experience, education, and skills. By fine-tuning BERT with domain-specific corpora, the model captures variations in resume structures and effectively understands contextual relationships between different sections.\n",
            "\n",
            "The content of the pargaph 15is: \n",
            "\n",
            "The content of the pargaph 16is: The results demonstrate that the BERT-based model achieves a classification accuracy of 92.5%, significantly outperforming traditional machine learning methods. Transfer learning proves essential for handling unstructured resumes, allowing the model to adapt to different writing styles. The study emphasizes the superiority of deep learning in parsing complex textual data, making it a viable approach for large-scale resume processing.\n",
            "\n",
            "The content of the pargaph 17is: \n",
            "\n",
            "The content of the pargaph 18is: Reference 3: End-to-End Resume Parsing and Finding Candidates for a Job Description using BERT\" by Vedant Bhatia et al. (2019)\n",
            "\n",
            "The content of the pargaph 19is: \n",
            "This paper presents a rule-based method for resume parsing, employing predefined templates and regular expressions to extract structured data. The system is designed to recognize standard resume formats and identify key sections such as contact information, education, and skills. By leveraging manually crafted heuristics, the approach ensures efficient text segmentation and classification.\n",
            "\n",
            "The content of the pargaph 20is: \n",
            "\n",
            "The content of the pargaph 21is: Despite achieving an accuracy of 85%, the rule-based system faces challenges when parsing highly unstructured or visually rich resumes. While it performs well on conventional formats, its scalability is limited due to its reliance on fixed patterns. The study underscores the need for adaptive models that can generalize across various resume styles.\n",
            "\n",
            "The content of the pargaph 22is: \n",
            "\n",
            "The content of the pargaph 23is: Reference4: \"Resume Parsing Using Machine Learning: A Comparative Study\"\n",
            "\n",
            "The content of the pargaph 24is: \n",
            "This study compares different machine learning models, including Support Vector Machines (SVMs), Random Forests, and Conditional Random Fields (CRFs), for resume parsing. Feature engineering techniques, such as n-grams and word embeddings, are used to enhance model performance. The research explores the impact of contextual representations in improving information extraction accuracy.\n",
            "\n",
            "The content of the pargaph 25is: \n",
            "\n",
            "The content of the pargaph 26is: Among the tested models, CRFs yield the highest F1-score of 0.91, particularly excelling in extracting job titles and skills. The study highlights the importance of feature selection in refining machine learning models for resume parsing. While CRFs offer robust performance, they require extensive domain-specific preprocessing to achieve optimal results.\n",
            "\n",
            "The content of the pargaph 27is: \n",
            "\n",
            "The content of the pargaph 28is: Reference5 : Case Study: NLP Based Resume Parser Using BERT in Python\"byPragnakalpTechlabs(2022)\n",
            "\n",
            "The content of the pargaph 29is: \n",
            "This paper introduces an automated pipeline for resume parsing that integrates NLP methods such as tokenization, lemmatization, and dependency parsing. The approach also incorporates clustering algorithms to group similar resumes, along with topic modeling techniques to extract key themes and skills from the text.\n",
            "\n",
            "The content of the pargaph 30is: \n",
            "\n",
            "The content of the pargaph 31is: The system achieves a precision of 88% in matching resumes with job descriptions, demonstrating its effectiveness in recruitment automation. Clustering proves useful in handling large datasets, while topic modeling provides deeper insights into candidate qualifications. The study underscores the growing role of NLP in optimizing hiring processes and improving candidate-job alignment.\n",
            "\n",
            "The content of the pargaph 32is:  \n",
            "\n",
            "The content of the pargaph 33is:  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s7zFvNPQbacx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}